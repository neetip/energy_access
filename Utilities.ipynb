{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import GPy\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.gaussian_process as gp\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression,Ridge,ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Positive, Interval\n",
    "from gpytorch.means import Mean\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "sns_c = sns.color_palette(palette='deep')\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper',font_scale=2)\n",
    "\n",
    "%run Utilities.ipynb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_spatial_v2(comreg_df,map_distances,threshold):\n",
    "    rnd_reg = np.random.choice(comreg_df['REG'].unique())\n",
    "    rnd_com = comreg_df.loc[comreg_df['REG'] == rnd_reg]['CCOD_CRCA'].sample(1).iloc[0]\n",
    "    f = 50 # 1 km = 0.00895305 degrees\n",
    "    data_partition = pd.DataFrame(comreg_df[['CCOD_CRCA','row_index']].copy(),index=comreg_df.index)\n",
    "    data_partition['type'] = 'train'\n",
    "    \n",
    "    training_inds = []\n",
    "    # starting from commune, add all other communes that are within rad to training and rest \n",
    "    #to test\n",
    "    while len(training_inds) < threshold:\n",
    "        f = f + 50\n",
    "        training_inds = [rnd_com]\n",
    "        testing_inds = []\n",
    "        rad = f * 0.00895305\n",
    "        training_inds = training_inds + list(map_distances.loc[\n",
    "            (map_distances['InputID'] == rnd_com) & (map_distances['Distance'] < rad)]['TargetID'])\n",
    "    testing_inds = list(set(list(comreg_df['CCOD_CRCA'])).difference(set(training_inds)))\n",
    "    data_partition.loc[data_partition['CCOD_CRCA'].isin(testing_inds),'type'] = 'test'\n",
    "\n",
    "    return data_partition, rnd_reg, rnd_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate_spatial(x,y,alpha,niters,classifier,args,map_distances,comreg_df):\n",
    "    selected_coms = []\n",
    "    selected_regs = []\n",
    "    #map to store the actual predictions\n",
    "    map_YPreds = []\n",
    "    map_YVar1 = []\n",
    "    map_YVar2 = []\n",
    "    map_YVar_final = []\n",
    "    \n",
    "    rmsevec = []\n",
    "    rmseurbanvec = []\n",
    "    rmseruralvec = []\n",
    "    rmseurbanvec_d = []\n",
    "    rmseurbanvec_nd = []\n",
    "    pcorrvec = []\n",
    "    scorrvec = []\n",
    "    ppvalvec = []\n",
    "    spvalvec = []\n",
    "\n",
    "    ny = y.shape[1]\n",
    "    s = args['s']\n",
    "    urtype = args['urtype'] # urban rural type\n",
    "   \n",
    "    ii = 0\n",
    "    while ii < niters:\n",
    "        # split data for crossvalidation\n",
    "        threshold = 225\n",
    "        #threshold = 330\n",
    "        data_partition,rnd_reg,rnd_com = stratified_spatial_v2(comreg_df,map_distances,threshold)\n",
    "        if ii % 10 == 0 and ii > 0:\n",
    "            print(\"Validation run %d of %d\"%(ii,niters))\n",
    "        ii = ii + 1\n",
    "        train_index = list(data_partition.loc[data_partition['type'] == 'train','row_index'].values)\n",
    "        test_index = list(data_partition.loc[data_partition['type'] == 'test','row_index'].values)\n",
    "        test_regions = data_partition.loc[data_partition['type'] == 'test','CCOD_CRCA'].str[0:2].astype(int).values\n",
    "        XTrain, XTest, STrain, STest = x[train_index,:], x[test_index,:], s[train_index,:], s[test_index,:]\n",
    "        urTrain, urTest = urtype[train_index,:],urtype[test_index,:]\n",
    "        YTrain, YTest = y[train_index,:], y[test_index,:]\n",
    "        if classifier in ['MultiViewGPRSpatial','Mixed' ,'Kriging','GPRSpatial']:\n",
    "            args['STrain'] = STrain\n",
    "            args['STest'] = STest\n",
    "        if classifier in ['GPRSpatial']:\n",
    "            args['urTrain'] = urTrain\n",
    "            args['urTest'] = urTest\n",
    "        try:\n",
    "            YPred,Vars = evaluate(XTrain,YTrain,XTest,alpha,classifier,args)\n",
    "            map_YPreds.append((test_index,YPred))\n",
    "            map_YVars = {}\n",
    "            if 'YVar' in Vars.keys():\n",
    "                YVar = Vars['YVar']\n",
    "                map_YVar_final.append((test_index,YVar))\n",
    "            if 'Var1' in Vars.keys():\n",
    "                YVar1 = Vars['Var1']\n",
    "                map_YVar1.append((test_index,YVar1))\n",
    "            if 'Var2' in Vars.keys():\n",
    "                YVar2 = Vars['Var2']\n",
    "                map_YVar2.append((test_index,YVar2))\n",
    "            selected_coms.append(rnd_com)\n",
    "            selected_regs.append(rnd_reg)\n",
    "        except NameError:\n",
    "            print(\"error identified\")\n",
    "            continue\n",
    "\n",
    "        rmse = np.zeros([ny,])\n",
    "        rmse[0:ny] = np.sqrt(np.mean((YPred[:,0:ny] - YTest)**2,axis=0))\n",
    "        rmserural = np.zeros([ny,])\n",
    "        rmseurban = np.zeros([ny,])\n",
    "        rmseurban_d = np.zeros([ny,]) # dakar\n",
    "        rmseurban_nd = np.zeros([ny,])# non-dakar\n",
    "\n",
    "        urbinds = np.where(urTest == 1)[0]\n",
    "        rurinds = np.where(urTest == 0)[0]\n",
    "        urb_dinds = np.array(set(urbinds).intersection(set(np.where(test_regions==1)[0])))\n",
    "        urb_ndinds = np.array(set(urbinds).intersection(set(np.where(test_regions!=1)[0])))\n",
    "  \n",
    "        rmseurban = np.sqrt(np.mean((YPred[urbinds,0:ny] - YTest[urbinds,:])**2,axis=0))\n",
    "        rmserural = np.sqrt(np.mean((YPred[rurinds,0:ny] - YTest[rurinds,:])**2,axis=0))\n",
    "        rmseurban_d = np.sqrt(np.mean((YPred[urb_dinds,0:ny] - YTest[urb_dinds,:])**2,axis=0))\n",
    "        rmseurban_nd = np.sqrt(np.mean((YPred[urb_ndinds,0:ny] - YTest[urb_ndinds,:])**2,axis=0))\n",
    "\n",
    "\n",
    "        pcorr = np.zeros([ny,])\n",
    "        scorr = np.zeros([ny,])\n",
    "        ppval = np.zeros([ny,])\n",
    "        spval = np.zeros([ny,])\n",
    "\n",
    "        for i in range(ny):\n",
    "            pcorr[i] = pearsonr(YPred[:,i],YTest[:,i])[0]\n",
    "            ppval[i] = pearsonr(YPred[:,i],YTest[:,i])[1]\n",
    "            scorr[i] = spearmanr(YPred[:,i],YTest[:,i])[0]\n",
    "            spval[i] = spearmanr(YPred[:,i],YTest[:,i])[1]\n",
    "        \n",
    "        pcorrvec.append(pcorr)\n",
    "        ppvalvec.append(ppval)\n",
    "        scorrvec.append(scorr)\n",
    "        spvalvec.append(spval)\n",
    "        rmsevec.append(rmse)\n",
    "        rmseurbanvec.append(rmseurban)\n",
    "        rmseruralvec.append(rmserural)\n",
    "        rmseurban_dvec.append(rmseurban_d)\n",
    "        rmseurban_ndvec.append(rmseurban_nd)\n",
    "\n",
    "\n",
    "    rmsevec = np.array(rmsevec)\n",
    "    rmseurbanvec = np.array(rmseurbanvec)\n",
    "    rmseruralvec = np.array(rmseruralvec)\n",
    "    rmseurban_dvec = np.array(rmseurban_dvec)\n",
    "    rmseurban_ndvec = np.array(rmseurban_ndvec)\n",
    "    pcorrvec = np.array(pcorrvec)\n",
    "    ppvalvec = np.array(ppvalvec)\n",
    "    scorrvec = np.array(scorrvec)\n",
    "    spvalvec = np.array(spvalvec)\n",
    "\n",
    "    return pcorrvec,ppvalvec,scorrvec,spvalvec,rmsevec,map_YPreds,map_YVar1,map_YVar2,map_YVar_final,rmseurbanvec,rmseruralvec,rmseurban_dvec,rmseurban_ndvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(XTrain,YTrain,XTest,alpha,classifier,args):\n",
    "    supported_methods = ['GPR','GPRSpatial','LinearRegression']\n",
    "    if classifier not in supported_methods:\n",
    "        print('Only following classifiers are supported')\n",
    "        print(supported_methods)\n",
    "        raise ValueError('Undefined classifier specified')\n",
    "        \n",
    "    Vars = {}\n",
    "    # learn model\n",
    "    if classifier == 'LinearRegression':\n",
    "        lmodel = LinearRegression()\n",
    "        lmodel.fit(XTrain,YTrain)\n",
    "        YPred = lmodel.predict(XTest)\n",
    "    elif classifier == 'GPR':\n",
    "        YPred,var = run_GP(XTrain,YTrain,XTest,alpha,[],[])\n",
    "        Vars['YVar'] = var\n",
    "    elif classifier == 'GPRSpatial':\n",
    "        STrain = args['STrain']\n",
    "        STest = args['STest']\n",
    "        urTrain = args['urTrain']\n",
    "        urTest = args['urTest']\n",
    "        if args['mokernel'] == True:\n",
    "            YPred,var = run_GPyTorchMO(XTrain,YTrain,XTest,alpha,STrain,STest,urTrain,urTest,\n",
    "                                       fitlinear=args['fitlinear'],\n",
    "                                       ard=args['ard'],mo_ard=args['mo_ard'],\n",
    "                                       initPhis=args['initPhis'])\n",
    "        else:\n",
    "            YPred,var = run_GPyTorch(XTrain,YTrain,XTest,alpha,STrain,STest,urTrain,urTest,\n",
    "                                     fitlinear=args['fitlinear'],ard=args['ard'])\n",
    "        Vars['YVar'] = var\n",
    "    else:\n",
    "        raise ValueError('Undefined model specified')\n",
    "    return YPred,Vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(a):\n",
    "    st = ''\n",
    "    for _n in a:\n",
    "        st = st+'%05.2f '%_n\n",
    "    print(st[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillMissingCommunes(current_predictions,all_commune_ids):\n",
    "    # find regional predictions\n",
    "    reg_predictions = {}\n",
    "    for c in current_predictions.keys():\n",
    "        r = c[0:2]\n",
    "        if r not in reg_predictions.keys():\n",
    "            reg_predictions[r] = []\n",
    "        reg_predictions[r].append(current_predictions[c])\n",
    "    avg_reg_predictions = {}\n",
    "    for r in reg_predictions.keys():\n",
    "        avg_reg_predictions[r] = np.mean(np.array(reg_predictions[r]),axis=0)\n",
    "    new_predictions = {}\n",
    "    for c in all_commune_ids:\n",
    "        if c in current_predictions.keys():\n",
    "            new_predictions[c] = current_predictions[c]\n",
    "        else:\n",
    "            new_predictions[c] = avg_reg_predictions[c[0:2]]\n",
    "    return new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredStats(map_YPreds,map_Yvar_combined,comreg_df,commune_ids):\n",
    "    ## Combining predicted means for each run to get average predictions for each commune\n",
    "    map_ComIndPreds = {}\n",
    "    for _l  in range(len(map_YPreds)):\n",
    "        #_l gives the index of the crossvalidation run\n",
    "        for _j in range(len(map_YPreds[_l][0])):\n",
    "            #_j gives the index of a commune\n",
    "            _com_id = comreg_df.loc[comreg_df['row_index'] == map_YPreds[_l][0][_j]]['CCOD_CRCA'].values[0]\n",
    "            if _com_id not in map_ComIndPreds.keys():\n",
    "                map_ComIndPreds[_com_id] = []\n",
    "            map_ComIndPreds[_com_id].append(map_YPreds[_l][1][_j])\n",
    "\n",
    "    map_ComIndMeanPreds = {}\n",
    "    for c in map_ComIndPreds.keys():\n",
    "        v = np.array(map_ComIndPreds[c])\n",
    "        map_ComIndMeanPreds[c] = np.mean(v,axis=0)\n",
    "    commune_ids = list(comreg_df['CCOD_CRCA'])\n",
    "    map_ComIndMeanPreds_filled = fillMissingCommunes(map_ComIndMeanPreds,commune_ids)\n",
    "    predictedMeans = pd.DataFrame.from_dict(map_ComIndMeanPreds,orient='index',columns=outputs)\n",
    "\n",
    "\n",
    "    if len(map_Yvar_combined) > 0:\n",
    "        ## Combining predicted Variances for each run to get average variance prediction for each commune\n",
    "        map_ComIndVars = {}\n",
    "        for _l  in range(len(map_Yvar_combined)):\n",
    "            #_l gives the index of the crossvalidation run\n",
    "            for _j in range(len(map_Yvar_combined[_l][0])):\n",
    "                #_j gives the index of a commune\n",
    "                _com_id = comreg_df.loc[comreg_df['row_index'] == map_YPreds[_l][0][_j]]['CCOD_CRCA'].values[0]\n",
    "                if _com_id not in map_ComIndVars.keys():\n",
    "                    map_ComIndVars[_com_id] = []\n",
    "                map_ComIndVars[_com_id].append(map_Yvar_combined[_l][1][_j])\n",
    "\n",
    "        map_ComIndMeanVars = {}\n",
    "        for c in map_ComIndVars.keys():\n",
    "            v = np.array(map_ComIndVars[c])\n",
    "            map_ComIndMeanVars[c] = np.mean(v,axis=0)\n",
    "    \n",
    "        map_ComIndMeanVars_filled = fillMissingCommunes(map_ComIndMeanVars,commune_ids)\n",
    "\n",
    "        predictedVars = pd.DataFrame.from_dict(map_ComIndMeanVars,orient='index',columns=outputs)\n",
    "    else:\n",
    "        predictedVars = pd.DataFrame.from_dict({})\n",
    "    return predictedMeans,predictedVars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLandsatData(year,resnetmodeltype='B'):\n",
    "    ## load landsat data\n",
    "    landsat = pickle.load(open('./landsatfeatures_{}_{}.pickle'.format(year,resnetmodeltype),'rb'))\n",
    "    landsat_means = {}\n",
    "    for c in landsat.keys():\n",
    "        landsat_means[c] = landsat[c][0]\n",
    "    landsat = pd.DataFrame.from_dict(landsat_means,orient='index')\n",
    "    return landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeaturesRaw(year,resnetmodeltype='B'):\n",
    "    ## load nightlight and aod data\n",
    "    df_nl_aod = pd.read_csv('nl_aod_{}_features.csv'.format(year))\n",
    "    df_nl_aod['Unnamed: 0'] = df_nl_aod['Unnamed: 0'].astype(str).str.zfill(8)\n",
    "    df_nl_aod.set_index('Unnamed: 0',inplace=True)\n",
    "    df_nl_aod = df_nl_aod[['nl_{}_avg'.format(year),\n",
    "                           'aod_{}_avg'.format(year),\n",
    "                           'nl_{}_var'.format(year),\n",
    "                           'aod_{}_var'.format(year),\n",
    "                           'pop{}'.format(year)]]\n",
    "\n",
    "    ## load landsat data\n",
    "    landsat = pickle.load(open('./landsatfeatures_{}_{}.pickle'.format(year,resnetmodeltype),'rb'))\n",
    "    landsat_means = {}\n",
    "    landsat_vars = {}\n",
    "    for c in landsat.keys():\n",
    "        landsat_means[c] = landsat[c][0]\n",
    "        landsat_vars[c] = landsat[c][1]\n",
    "    landsat = pd.DataFrame.from_dict(landsat_means,orient='index')\n",
    "    landsat_vars = pd.DataFrame.from_dict(landsat_vars,orient='index')\n",
    "    landsat = landsat.join(landsat_vars,rsuffix='_var')\n",
    "    # join everything\n",
    "    data_aug = landsat.join(df_nl_aod,how='left')\n",
    "    return data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAllData(year,pcamd,resnetmodeltype='B'):\n",
    "    \n",
    "    ## load nightlight and aod data\n",
    "    df_nl_aod = pd.read_csv('nl_aod_{}_features.csv'.format(year))\n",
    "    df_nl_aod['Unnamed: 0'] = df_nl_aod['Unnamed: 0'].astype(str).str.zfill(8)\n",
    "    df_nl_aod.set_index('Unnamed: 0',inplace=True)\n",
    "    df_nl_aod = df_nl_aod[['nl_{}_avg'.format(year),\n",
    "                           'aod_{}_avg'.format(year),\n",
    "                           'nl_{}_var'.format(year),\n",
    "                           'aod_{}_var'.format(year),\n",
    "                           'pop{}'.format(year)]]\n",
    "\n",
    "    ## load landsat data\n",
    "    landsat = pickle.load(open('./landsatfeatures_{}_{}.pickle'.format(year,resnetmodeltype),'rb'))\n",
    "    landsat_means = {}\n",
    "    landsat_vars = {}\n",
    "    for c in landsat.keys():\n",
    "        landsat_means[c] = landsat[c][0]\n",
    "        landsat_vars[c] = landsat[c][1]\n",
    "    landsat = pd.DataFrame.from_dict(landsat_means,orient='index')\n",
    "    landsat_vars = pd.DataFrame.from_dict(landsat_vars,orient='index')\n",
    "\n",
    "    \n",
    "    ## use pca to embed into lower dimensional space\n",
    "    ls_cols = ['ls_{}'.format(i+1) for i in np.arange(pcamd.n_components)]\n",
    "    landsat_sub = pd.DataFrame(pcamd.fit_transform(landsat.values),\n",
    "                           index=landsat.index,columns=ls_cols)\n",
    "    \n",
    "    # calculate variance for each PCA feature\n",
    "    ls_var_cols = []\n",
    "    for i in range(n_components):\n",
    "        ls_var_cols.append('ls_var_'+str(i+1))\n",
    "    landsat_sub_vars = pd.DataFrame(np.dot(landsat_vars,np.power(pcamd.components_,2).T),\n",
    "                                    index=landsat.index,columns=ls_var_cols)\n",
    "    landsat_sub = landsat_sub.join(landsat_sub_vars)\n",
    "    \n",
    "    # load urban rural mapping\n",
    "    urbrur = pd.read_csv('./communes_urban_rural.csv')\n",
    "    urbrur['CCOD_CRCA'] = urbrur['CCOD_CRCA'].astype(str).str.zfill(8)\n",
    "    urbrur.set_index('CCOD_CRCA',inplace=True)\n",
    "    urbrur = pd.DataFrame(urbrur['TYPE'])\n",
    "\n",
    "    ccoords = pd.read_csv('./urban_rural_points.csv',\n",
    "                                      index_col='CCOD_CRCA')\n",
    "    ccoords.index = ccoords.index.astype(str).str.zfill(8)\n",
    "    \n",
    "    # join everything\n",
    "    data_aug = landsat_sub.join(df_nl_aod,\n",
    "                            how='left').join(ccoords,\n",
    "                                             how='left').join(urbrur,\n",
    "                                                              how='left')\n",
    "    return data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,ard=False,feat_kernel_type='rbf',hasTime=False):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        \n",
    "        if hasTime:\n",
    "            n_feats = train_x.shape[1] - 4\n",
    "        else:\n",
    "            n_feats = train_x.shape[1] - 3\n",
    "        if ard:\n",
    "            ard_num_dims = n_feats\n",
    "        else:\n",
    "            ard_num_dims = None\n",
    "        #self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        #self.mean_module = LinearMeanBasis(n_feats)\n",
    "        if feat_kernel_type == 'matern':\n",
    "            kern_feat = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(\n",
    "                nu=1.5,ard_num_dims=ard_num_dims,active_dims=tuple(range(n_feats))))\n",
    "        else:\n",
    "            kern_feat = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(\n",
    "                ard_num_dims=ard_num_dims,active_dims=tuple(range(n_feats))))\n",
    "        curr_ind = n_feats\n",
    "        if hasTime:\n",
    "            kern_time = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(active_dims=tuple([curr_ind])))\n",
    "            curr_ind += 1\n",
    "        kern_spat = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(active_dims=tuple([curr_ind,curr_ind+1])))\n",
    "        kern_ur = gpytorch.kernels.RBFKernel(active_dims=tuple([curr_ind+2]))\n",
    "        self.likelihood.noise_covar.noise = 1\n",
    "        # initialize hyper-parameters\n",
    "        kern_feat.outputscale = 1\n",
    "        kern_feat.base_kernel.lengthscale = 0.25\n",
    "        kern_spat.outputscale = 1\n",
    "        kern_spat.base_kernel.lengthscale = 0.25\n",
    "        kern_ur.lengthscale = 1e-6\n",
    "        if hasTime:\n",
    "            kern_time.outputscale = 1\n",
    "            kern_time.base_kernel.lengthscale = 0.25\n",
    "        # do not optimize the urban-rural kernel's lengthscale\n",
    "        kern_ur.raw_lengthscale.requires_grad_(False)\n",
    "        if hasTime:\n",
    "            self.covar_module = kern_feat + (kern_spat * kern_ur) + kern_time\n",
    "        else:\n",
    "            self.covar_module = kern_feat + (kern_spat * kern_ur)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel for energy work\n",
    "class ExactGPMOModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood,num_tasks,\n",
    "                 ard=False,mo_ard=False,feat_kernel_type='rbf',initPhis=None,hasTime=False):\n",
    "        super(ExactGPMOModel, self).__init__(train_x, train_y, likelihood)\n",
    "        \n",
    "        if hasTime:\n",
    "            n_feats = train_x.shape[1] - 5\n",
    "        else:\n",
    "            n_feats = train_x.shape[1] - 4\n",
    "        \n",
    "        if ard:\n",
    "            ard_num_dims = n_feats\n",
    "        else:\n",
    "            ard_num_dims = None\n",
    "            \n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        if feat_kernel_type == 'matern':\n",
    "            kern_feat = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(\n",
    "                nu=1.5,ard_num_dims=ard_num_dims,active_dims=tuple(range(n_feats))))\n",
    "        else:\n",
    "            kern_feat = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(\n",
    "                ard_num_dims=ard_num_dims,active_dims=tuple(range(n_feats))))\n",
    "        curr_ind = n_feats\n",
    "        if hasTime:\n",
    "            kern_time = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(active_dims=tuple([curr_ind])))\n",
    "            curr_ind += 1\n",
    "        kern_spat = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(active_dims=tuple([curr_ind,curr_ind+1])))\n",
    "        kern_ur = gpytorch.kernels.RBFKernel(active_dims=tuple([curr_ind+2]))\n",
    "        kern_mo = MOKernel(num_tasks=num_tasks,active_dims=tuple([curr_ind+3]),ard=mo_ard)\n",
    "\n",
    "        self.likelihood.noise_covar.noise = 1\n",
    "        # initialize hyper-parameters\n",
    "        kern_feat.outputscale = 1\n",
    "        kern_feat.base_kernel.lengthscale = 0.25\n",
    "        kern_spat.outputscale = 1\n",
    "        kern_spat.base_kernel.lengthscale = 0.25\n",
    "        if hasTime:\n",
    "            kern_time.outputscale = 1\n",
    "            kern_time.base_kernel.lengthscale = 0.25\n",
    "        kern_ur.lengthscale = 1e-6\n",
    "        if initPhis != None:\n",
    "            kern_mo.phis = initPhis\n",
    "        \n",
    "        # do not optimize the urban-rural kernel's lengthscale\n",
    "        kern_ur.raw_lengthscale.requires_grad_(False)\n",
    "        if hasTime:\n",
    "            self.covar_module = (kern_feat + (kern_spat * kern_ur) + kern_time)*kern_mo # MAIN temporal kernel\n",
    "        else:\n",
    "            self.covar_module = (kern_feat + (kern_spat * kern_ur))*kern_mo # MAIN kernel no time\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_GPyTorch(xtrain,ytrain,xtest,alpha,strain=[],stest=[],urtrain=[],urtest=[],\n",
    "           fitlinear=False,return_models=False,ard=False,hasTime=False,missingvalue=-999):\n",
    "    res_train = []\n",
    "    if fitlinear:\n",
    "        # fit linear model\n",
    "        linearmodel = []\n",
    "        y_hat_test = []\n",
    "        for oindex in range(ytrain.shape[1]):\n",
    "            lm = ElasticNet(alpha = alpha)\n",
    "            minds = np.where(ytrain[:,oindex]!=missingvalue)[0]\n",
    "            xt = xtrain[minds,:]\n",
    "            yt = ytrain[minds,oindex:oindex+1]\n",
    "            lm.fit(xt,yt)\n",
    "            # get predictions for training data and test data using the linear model\n",
    "            y_h_train = lm.predict(xt)\n",
    "            y_h_test = lm.predict(xtest)\n",
    "            linearmodel.append(lm)\n",
    "            res_train.append(yt.flatten() - y_h_train)\n",
    "            y_hat_test.append(y_h_test)\n",
    "        y_hat_test = np.vstack(y_hat_test).T\n",
    "    else:\n",
    "        for oindex in range(ytrain.shape[1]):\n",
    "            minds = np.where(ytrain[:,oindex]!=missingvalue)[0]\n",
    "            res_train.append(ytrain[minds,oindex])\n",
    "    ystar_test = np.zeros([xtest.shape[0],ytrain.shape[1]])\n",
    "    ystar_var = np.zeros([xtest.shape[0],ytrain.shape[1]])\n",
    "    if return_models:\n",
    "        models = []\n",
    "    totaltrainingsize = 0\n",
    "    for oindex in range(ytrain.shape[1]):\n",
    "        minds = np.where(ytrain[:,oindex]!=missingvalue)[0]\n",
    "        xtrain_aug = np.hstack([xtrain[minds,:],strain[minds,:],urtrain[minds,:]])\n",
    "        xtest_aug = np.hstack([xtest,stest,urtest])\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        xtrain_aug_tensor = torch.tensor(np.array(xtrain_aug,dtype='float32'))\n",
    "        ytrain_tensor = torch.tensor(np.array(res_train[oindex],dtype='float32'))\n",
    "\n",
    "        totaltrainingsize += xtrain_aug_tensor.shape[0]\n",
    "        model = ExactGPModel(xtrain_aug_tensor, \n",
    "                             ytrain_tensor, \n",
    "                             likelihood, ard,hasTime=hasTime)\n",
    "        train_GPyTorch(model,likelihood,xtrain_aug_tensor,ytrain_tensor,iterations=100)\n",
    "        \n",
    "        if return_models:\n",
    "            models.append(model)\n",
    "        # get predictions\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        _rstar = likelihood(model(torch.tensor(np.array(xtest_aug,dtype='float32'))))\n",
    "        \n",
    "        ystar_test[:,oindex] = _rstar.mean.detach().numpy().ravel()\n",
    "        ystar_var[:,oindex] = _rstar.variance.detach().numpy().ravel()\n",
    "    if fitlinear:\n",
    "        ystar_test = y_hat_test + ystar_test\n",
    "    if return_models:\n",
    "        if fitlinear:\n",
    "            return ystar_test,ystar_var,(models,linearmodel)\n",
    "        else:\n",
    "            return ystar_test,ystar_var,(models)\n",
    "    else:\n",
    "        return ystar_test,ystar_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_GPyTorchMO(xtrain,ytrain,xtest,alpha,strain=[],stest=[],urtrain=[],urtest=[],\n",
    "           fitlinear=False,return_models=False,ard=False,mo_ard=False,initPhis=None,hasTime=False):\n",
    "    if fitlinear:\n",
    "        # fit linear model\n",
    "        linearmodel = ElasticNet(alpha = alpha)\n",
    "        linearmodel.fit(xtrain,ytrain)\n",
    "        # get predictions for training data and test data using the linear model\n",
    "        y_hat_train = linearmodel.predict(xtrain)\n",
    "        y_hat_test = linearmodel.predict(xtest)\n",
    "        # get the residual terms for output\n",
    "        res_train = ytrain - y_hat_train\n",
    "    else:\n",
    "        res_train = ytrain\n",
    "    ystar_test = np.zeros([xtest.shape[0],ytrain.shape[1]])\n",
    "    ystar_var = np.zeros([xtest.shape[0],ytrain.shape[1]])\n",
    "    \n",
    "    # prepare the training and test data with one output column \n",
    "    # and one additional column in input indicating the output index\n",
    "    xtrain_aug = np.hstack([xtrain,strain,urtrain])\n",
    "    xtest_aug = np.hstack([xtest,stest,urtest])\n",
    "    xtrain_f,res_train_f = flattenMO(xtrain_aug,res_train.shape[1],res_train)\n",
    "    xtest_f = flattenMO(xtest_aug,res_train.shape[1])\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    xtrain_f_tensor = torch.tensor(np.array(xtrain_f,dtype='float32'))\n",
    "    ytrain_f_tensor = torch.tensor(np.array(res_train_f,dtype='float32')).flatten()\n",
    "    xtest_f_tensor = torch.tensor(np.array(xtest_f,dtype='float32'))\n",
    "    \n",
    "    model = ExactGPMOModel(xtrain_f_tensor, \n",
    "                         ytrain_f_tensor, \n",
    "                         likelihood, ytrain.shape[1], \n",
    "                         ard=ard,mo_ard=mo_ard,initPhis=initPhis,hasTime=hasTime)\n",
    "    train_GPyTorch(model,likelihood,xtrain_f_tensor,ytrain_f_tensor,iterations=100,verbose=False)\n",
    "    # get predictions\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    _rstar = likelihood(model(xtest_f_tensor))\n",
    "    ystar_f_test = _rstar.mean.detach().numpy().ravel()\n",
    "    ystar_f_var = _rstar.variance.detach().numpy().ravel()\n",
    "    # unflatten ystar_f_test\n",
    "    ystar_test = unflattenMO(ystar_f_test,ytrain.shape[1])\n",
    "    ystar_var = unflattenMO(ystar_f_var,ytrain.shape[1])\n",
    "\n",
    "\n",
    "    if fitlinear:\n",
    "        ystar_test = y_hat_test + ystar_test\n",
    "\n",
    "    if return_models:\n",
    "        if fitlinear:\n",
    "            return ystar_test,ystar_var,(model,linearmodel)\n",
    "        else:\n",
    "            return ystar_test,ystar_var,(model)\n",
    "    else:\n",
    "        return ystar_test,ystar_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenMO(X,ny,y=[]):\n",
    "    Xf = np.vstack([X]*ny)\n",
    "    I = np.repeat(np.arange(0,ny),X.shape[0]).reshape(-1,1)\n",
    "    Xf = np.hstack([Xf,I])\n",
    "    if len(y) > 0:\n",
    "        yf = y.reshape(-1,1,order='F')\n",
    "        return Xf,yf\n",
    "    else:\n",
    "        return Xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflattenMO(y,ny):\n",
    "    return y.reshape(int(y.shape[0]/ny),ny,order='F')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOKernel(gpytorch.kernels.Kernel):\n",
    "    is_stationary = False\n",
    "    has_lengthscale = False\n",
    "\n",
    "    # We will register the parameter when initializing the kernel\n",
    "    def __init__(self,\n",
    "                 phis_constraint=None,\n",
    "                 taus_constraint=None,\n",
    "                 num_tasks=4,\n",
    "                 ard=False,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # register parameters\n",
    "        # taus\n",
    "        if ard:\n",
    "            num_ard_dims = num_tasks\n",
    "        else:\n",
    "            num_ard_dims = 1\n",
    "        self.register_parameter(name='raw_taus',\n",
    "                                parameter=torch.nn.Parameter(\n",
    "                                    torch.ones(*self.batch_shape,1,num_ard_dims))\n",
    "                               )\n",
    "        if taus_constraint is None:\n",
    "            taus_constraint = Positive()\n",
    "        self.register_constraint('raw_taus',taus_constraint)\n",
    "        # NOTE - not setting any prior on the taus parameter yet\n",
    "        # phis\n",
    "        self.register_parameter(name='raw_phis',\n",
    "                                parameter=torch.nn.Parameter(\n",
    "                                    torch.ones(*self.batch_shape,\n",
    "                                                1,int(0.5*num_tasks*(num_tasks-1)))\n",
    "                                )\n",
    "                               )\n",
    "        if phis_constraint is None:\n",
    "            # contraint the angles to be between -pi and +pi\n",
    "            #phis_constraint = Interval(-3.141,3.141)\n",
    "            phis_constraint = Interval(0,3.141)\n",
    "        self.register_constraint('raw_phis',phis_constraint)\n",
    "        # NOTE - not setting any prior on the phis parameter yet\n",
    "        self.num_tasks = num_tasks\n",
    "                \n",
    "    @property\n",
    "    def phis(self):\n",
    "        return self.raw_phis_constraint.transform(self.raw_phis)\n",
    "    @phis.setter\n",
    "    def phis(self,value):\n",
    "        return self._set_phis(value)\n",
    "    \n",
    "    def _set_phis(self,value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_phis)\n",
    "        self.initialize(raw_phis=self.raw_phis_constraint.inverse_transform(value))\n",
    "        \n",
    "    @property\n",
    "    def taus(self):\n",
    "        return self.raw_taus_constraint.transform(self.raw_taus)\n",
    "    @taus.setter\n",
    "    def taus(self,value):\n",
    "        return self._set_taus(value)\n",
    "    \n",
    "    def _set_taus(self,value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_taus)\n",
    "        self.initialize(raw_taus=self.raw_taus_constraint.inverse_transform(value))\n",
    "        \n",
    "\n",
    "    # this is the kernel function\n",
    "    def forward(self, x1, x2, **params):\n",
    "        # construct the S and W matrices\n",
    "        self.L = getSpherical(self.num_tasks,self.phis)\n",
    "        if self.taus.shape[1] == 1:\n",
    "            self.W = self.taus.flatten()*torch.matmul(self.L.t(),self.L)\n",
    "        else:\n",
    "            self.W = torch.matmul(torch.matmul(self.L.t(),self.L),\n",
    "                              torch.diag(self.taus.flatten()))\n",
    "        x1_int = x1.type(torch.LongTensor)\n",
    "        x2_int = x2.type(torch.LongTensor)\n",
    "        grid_x1, grid_x2 = torch.meshgrid(x1_int.flatten(), x2_int.flatten())\n",
    "        inds = torch.vstack([grid_x1.flatten(),grid_x2.flatten()])\n",
    "        Kmat = self.W[inds[0,:],inds[1:]].reshape(\n",
    "            [x1.shape[0],x2.shape[0]])\n",
    "        return Kmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GPyTorch(model,likelihood,train_x,train_y,iterations=100,verbose=False):\n",
    "    model = model.train()\n",
    "    likelihood = likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    for i in range(iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "\n",
    "        loss.backward()\n",
    "        if verbose:\n",
    "            print('Iter %d/%d - Loss: %.3f' % (i + 1, iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "    if verbose:\n",
    "        print('Optimization done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rectangular_torch(inp):\n",
    "    r = inp[0]\n",
    "    sz = inp.shape[0]\n",
    "    multi_sin = 1\n",
    "    if sz == 1:\n",
    "        convert=torch.zeros([2],dtype=inp.dtype,device=inp.device)\n",
    "    else:\n",
    "        convert=torch.zeros([sz],dtype=inp.dtype,device=inp.device)\n",
    "    for i in range(1, sz-1):\n",
    "        convert = convert.put_(torch.tensor([i-1],device=inp.device),r* multi_sin * torch.cos(inp[i]))\n",
    "        multi_sin = multi_sin*torch.sin(inp[i])\n",
    "    convert = convert.put_(torch.tensor([-2],device=inp.device),r* multi_sin *torch.cos(inp[-1]))\n",
    "    convert = convert.put_(torch.tensor([-1],device=inp.device),r* multi_sin *torch.sin(inp[-1]))\n",
    "    return convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_spherical_torch(inp):\n",
    "    result =[]\n",
    "\n",
    "    for element in range(0, len(inp)):\n",
    "        r = 0\n",
    "        for i in range(0, len(inp[element])):\n",
    "            r += inp[element][i]*inp[element][i]\n",
    "        r = math.sqrt(r)\n",
    "        convert = [r]\n",
    "        for i in range (0 ,len(inp[element])-2):\n",
    "            convert.append(round(math.acos(inp[element][i] / r),6))\n",
    "            r = math.sqrt(r*r - inp[element][i]*inp[element][i])\n",
    "        if(inp[element][-2] >= 0):\n",
    "            convert.append(math.acos(inp[element][-2]/r))\n",
    "        else:\n",
    "            convert.append(2*math.pi - math.acos(inp[element][-2] /r))\n",
    "        convert = torch.from_numpy(np.array(convert))\n",
    "        result += [convert]\n",
    "    result = torch.stack(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSpherical(n_tasks,phis):\n",
    "\n",
    "    phis_mat = torch.tril(torch.ones([n_tasks,n_tasks],dtype=torch.float32,device=phis.device))\n",
    "    inds = torch.tril_indices(n_tasks-1,n_tasks-1)+1\n",
    "    phis_mat = phis_mat.index_put_((inds[0,:],inds[1,:]),phis.flatten()).T\n",
    "    # prepare the vectorized form of the output matrix\n",
    "    vec = torch.ones([1,int(0.5*n_tasks*(n_tasks+1))],device=phis.device)\n",
    "    for n in range(1,n_tasks):\n",
    "        st = torch.sum(torch.arange(n+1))\n",
    "        en = torch.sum(torch.arange(n+2))\n",
    "        rng = torch.arange(st,en)\n",
    "        vec = vec.index_put_((torch.zeros(rng.shape[0],dtype=torch.long),rng), \n",
    "                             convert_rectangular_torch(phis_mat[0:n+1,n]))\n",
    "\n",
    "    S = torch.zeros([n_tasks,n_tasks],device=phis.device)\n",
    "    inds = torch.tril_indices(n_tasks,n_tasks)\n",
    "    S = S.index_put_((inds[0,:],inds[1,:]),vec.flatten()).T\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPhis(W):\n",
    "    L = torch.cholesky(W).t()\n",
    "    n_tasks = W.shape[1]\n",
    "    vec = torch.ones([1,int(0.5*n_tasks*(n_tasks-1))],device=L.device)\n",
    "    st = torch.tensor(0)\n",
    "    for n in range(1,n_tasks):\n",
    "        en = torch.sum(torch.arange(n+1))\n",
    "        rng = torch.arange(st,en)\n",
    "        rect = convert_spherical_torch(L[0:(n+1),n].reshape((1,n+1)))[0,1:].float()\n",
    "        vec = vec.index_put_((torch.zeros(rng.shape[0],dtype=torch.long),rng), \n",
    "                             rect)\n",
    "        st = en\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScatter(pred_means_df,pred_vars_df,year,target,pop,outputfilename=None):\n",
    "    pred_means_df = pred_means_df.join(pd.DataFrame(pop))\n",
    "    # aggregate at region+type level\n",
    "    pred = pred_means_df.groupby(['region_TYPE']).mean()\n",
    "    pred['pop{}'.format(year)] = pred_means_df.groupby(['region_TYPE'])['pop{}'.format(year)].sum()\n",
    "    pred.name = 'Predicted'\n",
    "    vars_ = pred_vars_df.groupby(['region_TYPE']).mean()\n",
    "    vars_.name = 'Predicted Variance'\n",
    "\n",
    "    # Validating using DHS data\n",
    "    df_dhs = pd.read_csv('dhs_data/{}_targets/targets_region.csv'.format(year))\n",
    "    df_dhs.set_index('Unnamed: 0',inplace=True)\n",
    "    result = df_dhs.join(pred,lsuffix='_true',rsuffix='_pred').join(vars_,rsuffix='_var')\n",
    "    result['type'] = result.index.str[3:]\n",
    "    \n",
    "    x_ = result.loc[:,'{}_true'.format(target)].values\n",
    "    y_ = result.loc[:,'{}_pred'.format(target)].values\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.set_xlabel('Survey-measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    \n",
    "    x2_ = sm.add_constant(x_) \n",
    "    est = sm.OLS(y_, x2_)\n",
    "    est2 = est.fit()\n",
    "    #print(est2.summary())\n",
    "    \n",
    "    sconst = 0.001\n",
    "    # make scatter\n",
    "    ax.scatter(result.loc[result['type']=='urban','{}_true'.format(target)],\n",
    "           result.loc[result['type']=='urban','{}_pred'.format(target)],\n",
    "           alpha=1,\n",
    "           c='r',s=64)#,\n",
    "           #s=sconst*(result.loc[result['type']=='urban','pop{}'.format(year)]))\n",
    "    \n",
    "    ax.scatter(result.loc[result['type']=='rural','{}_true'.format(target)],\n",
    "           result.loc[result['type']=='rural','{}_pred'.format(target)],\n",
    "           alpha=1,\n",
    "           c='b',s=64)#,\n",
    "           #s=sconst*(result.loc[result['type']=='urban','pop{}'.format(year)]))\n",
    "    ax.set_xlim([-0.1,1])\n",
    "    ax.set_ylim([-0.1,1])\n",
    "    ax.legend(['Urban','Rural'],fontsize=20,loc=\"lower right\",markerscale=1)#0.5),bbox_to_anchor=(1.35, 0))\n",
    "    # add population legend\n",
    "    #ps = []\n",
    "    #ls = [50000,100000,500000,1000000]\n",
    "    #for p in ls:\n",
    "    #    ps.append(ax.scatter([], [], c='gray', alpha=0.8, s=p*sconst,label=str(p)))\n",
    "    #leg = Legend(ax, ps,['50k','100k','500k','1000k'],fontsize=20,\n",
    "    #             loc='center right', frameon=True, markerscale=1,\n",
    "    #             title='Population\\n({})'.format(year),labelspacing=1,bbox_to_anchor=(1.35, 0.5))\n",
    "    #ax.add_artist(leg);\n",
    "\n",
    "    #regression line\n",
    "    x_test = np.linspace(0,1,10)#np.sort(x_)\n",
    "    ax.plot(x_test, \n",
    "            LinearRegression().fit(\n",
    "                x_[:,np.newaxis], y_[:,np.newaxis]\n",
    "            ).predict(x_test[:,np.newaxis]),\n",
    "            color='k'\n",
    "           )\n",
    "    ax.text(0.1,0.8, '$R^2$ = {:.2f}'.format(est2.rsquared), fontsize=20)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if outputfilename != None:\n",
    "        fig.savefig(outputfilename,dpi=192)\n",
    "    return est2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepPredictions(YPred,var,data_df,outputs,urtest):\n",
    "    # prepare predictions for evaluation\n",
    "    \n",
    "    pred_means_df = pd.DataFrame(YPred,index=data_df.index,columns=outputs)\n",
    "    pred_vars_df = pd.DataFrame(var,index=data_df.index,columns=outputs)\n",
    "\n",
    "    pred_means_df = pred_means_df.join(pd.DataFrame(urtest.map({0:'rural',1:'urban'})))\n",
    "    pred_means_df['region'] = pred_means_df.index.str[0:2]\n",
    "    pred_means_df['region_TYPE'] = pred_means_df['region']+'_'+pred_means_df['TYPE']\n",
    "\n",
    "    pred_vars_df = pred_vars_df.join(pd.DataFrame(urtest.map({0:'rural',1:'urban'})))\n",
    "    pred_vars_df['region'] = pred_vars_df.index.str[0:2]\n",
    "    pred_vars_df['region_TYPE'] = pred_vars_df['region']+'_'+pred_vars_df['TYPE']\n",
    "    \n",
    "    return pred_means_df,pred_vars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateResults(pred_means_df,pred_vars_df,rmsevals,varvals,year,target):\n",
    "    # evaluate at region+type level\n",
    "    pred = pred_means_df.groupby(['region_TYPE']).mean()\n",
    "    pred.name = 'Predicted'\n",
    "    vars_ = pred_vars_df.groupby(['region_TYPE']).mean()\n",
    "    vars_.name = 'Predicted Variance'\n",
    "\n",
    "    # Validating using DHS data\n",
    "    df_dhs = pd.read_csv('dhs_data/{}_targets/targets_region.csv'.format(year))\n",
    "    df_dhs.set_index('Unnamed: 0',inplace=True)\n",
    "    result = df_dhs.join(pred,lsuffix='_true',rsuffix='_pred').join(vars_,rsuffix='_var')\n",
    "    result['type'] = result.index.str[3:]\n",
    "\n",
    "    t = target\n",
    "    tmp_df = result.loc[:,['{}_true'.format(t),'{}_pred'.format(t),'{}'.format(t),'type']]\n",
    "    tmp_df['sqdiff'] = (tmp_df.loc[:,'{}_true'.format(t)] - tmp_df.loc[:,'{}_pred'.format(t)])**2\n",
    "    \n",
    "    r_ = np.sqrt(tmp_df.groupby('type')['sqdiff'].mean())\n",
    "    p_ = pearsonr(tmp_df.loc[:,'{}_true'.format(t)],tmp_df.loc[:,'{}_pred'.format(t)])\n",
    "    s_ = spearmanr(tmp_df.loc[:,'{}_true'.format(t)],tmp_df.loc[:,'{}_pred'.format(t)])\n",
    "    v_ = tmp_df.groupby('type')[t].mean()\n",
    "    \n",
    "    rmsevals[t] = {'rmse':r_,'pearsonr':p_,'spearmanr':s_,'variance':v_}\n",
    "    \n",
    "    return rmsevals,varvals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
